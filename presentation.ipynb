{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bag Of Words --> Transferlearning\n",
    "by Niels Helsø \n",
    "git = https://github.com/slein89/BOW_transferlearning  \n",
    "linkedin = https://www.linkedin.com/in/nielshelsoe/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Purpose\n",
    "- 3 models for NLP\n",
    "- compare\n",
    "- every model have a tutorial \n",
    "- IT IS ALL ON **GIT HUB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "1. Introduction\n",
    "2. Classifier with BoW (Bag Of Words)\n",
    "3. Classifier with Word embeddings\n",
    "4. Classifier with Transferlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code, articles, tutorials and stuff\n",
    "Git url = https://github.com/slein89/BOW_transferlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "- Who am I?\n",
    "    - Name = Niels\n",
    "    - Age = 29\n",
    "    - Education = cand. it\n",
    "    - have coded in python for 2 years time\n",
    "    - Love NLP\n",
    "    - Favorite guilty pleasure = paagen gifler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Danish Business Authority\n",
    "- Government agency\n",
    "- 550 people\n",
    "- Goal: The best conditions and framework for Danish companies\n",
    "- Three locations: Copenhagen, Silkeborg, Nykøbing Falster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data science in Danish Business Authority\n",
    "- Data-Science team of 5 (WE ARE HIRING!)\n",
    "- Use Data science \n",
    "- Profiles:\n",
    "    - fine arts\n",
    "    - economics\n",
    "    - engineer\n",
    "    - phd\n",
    "    - it\n",
    "    - information studies\n",
    "- Using Python and libs such as:\n",
    "    - sklearn\n",
    "    - pandas\n",
    "    - keras\n",
    "    - numpy\n",
    "    - and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# why this talk?\n",
    "1. Give an overview of where natural language processing(NLP) is going and where it has been\n",
    "2. Pro and cons of three differrent methods\n",
    "3. Demystify NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# PLEASE ASK QUESTIONS \n",
    "There is no such thing as a dumb question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Disclaimer\n",
    "Use this at your own risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data \n",
    "thanks to Prayson Wilfred Daniel\n",
    "- trustpilot reviews\n",
    "- 254464 (after making a equal distribution of the two classes)\n",
    "- task: build a sentiment classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of the to classes\n",
      "-----------------\n",
      "1    127232\n",
      "0    127232\n",
      "Name: y, dtype: int64\n",
      "\n",
      "Examples of text\n",
      "-----------------\n",
      "det var iorden \n",
      "nåede sikkert og hurtigt frem det var en god oplevelse\n",
      "vi har altid været glade for tryg og deres måde at håndtere forsikringssager på vi anmelder self heller ikke hvad som helst men til gengæld får vi altid en ærlig og redelig behandling uden problemer selv om tryg i perioder kan være lidt dyrere end andre forsikringsselskaber foretrækker vi dem fordi vi så heller ikke skal trækkes med diskussioner om udbetaling når uheldet er ude \n",
      "hurtig pakke lev til pakkeboksen\n",
      "bestilte tøj på nettet og fik pakken hurtigere end ventet og kunne endda følge den hele vejen \n"
     ]
    }
   ],
   "source": [
    "from src.data.load_data import load_trustpilot_data\n",
    "from src.preprocessing.text_pre import clean_text\n",
    "df_trust = load_trustpilot_data()\n",
    "df_trust = clean_text(df_trust, 'text')\n",
    "print('Total value of classes amount to')\n",
    "print('-----------------')\n",
    "print(df_trust['y'].value_counts())\n",
    "print('')\n",
    "print('Examples of text')\n",
    "print('-----------------')\n",
    "for index,row in df_trust[:5].iterrows():\n",
    "    print(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load src/preprocessing/text_pre.py\n",
    "import re\n",
    "import string\n",
    "def clean_text (df, row_name):\n",
    "    #lower all text\n",
    "    df[row_name] = df[row_name].str.lower()\n",
    "    #remove all numbers\n",
    "    df[row_name] = df[row_name].apply(lambda x: x.translate(str.maketrans('','','0123456789')))\n",
    "    #remove all special chareters\n",
    "    df[row_name] = df[row_name].apply(lambda x: re.sub('[\\W]+', ' ', x))\n",
    "    #make translation tabel where punctuation is removed and apply it to the text\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    df[row_name] = df[row_name].apply(lambda x: x.translate(table))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Metrics:\n",
    "- Roc auc\n",
    "- Confusion Matrix\n",
    "- Classification report (recall, precision, F1 score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BoW (Bag Of Words)\n",
    "- one of the first NLP approaches\n",
    "- converts text to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Libs for BoW in python\n",
    "- Gensim\n",
    "- Nltk\n",
    "- Spacy\n",
    "- Sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "corpus=['Pydatacopenhagen is the best place to be tonight, yes tonight',\n",
    "'At Pydatacopenhagen every one is welcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 1 1 1 2 0 1]\n",
      " [1 0 0 1 1 1 0 1 0 0 0 1 0]]\n",
      "{'pydatacopenhagen': 7, 'is': 4, 'the': 8, 'best': 2, 'place': 6, 'to': 9, 'be': 1, 'tonight': 10, 'yes': 12, 'at': 0, 'every': 3, 'one': 5, 'welcome': 11}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "#the vector representation of the words (13)\n",
    "print( vectorizer.fit_transform(corpus).todense() )\n",
    "#count of the uniq words in corpus\n",
    "print( vectorizer.vocabulary_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ngrams\n",
    "- helps with preserving some meaning to the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pydatacopenhagen', 'is')\n",
      "('is', 'the')\n",
      "('the', 'best')\n",
      "('best', 'place')\n",
      "('place', 'to')\n",
      "('to', 'be')\n",
      "('be', 'tonight,')\n",
      "('tonight,', 'yes')\n",
      "('yes', 'tonight')\n"
     ]
    }
   ],
   "source": [
    "corpus=['Pydatacopenhagen is the best place to be tonight, yes tonight',\n",
    "'At Pydatacopenhagen everyone is welcome']\n",
    "from nltk import ngrams\n",
    "grams = ngrams(corpus[0].split(),2)\n",
    "for gram in grams:\n",
    "    print (gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.preprocessing.metrics import get_metrics\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_trust['text'], df_trust['y'], random_state=42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### performanche on BoW\n",
    "Let's take a look at a tutorial and see how BOW performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score\n",
      "0.9001972503652068\n",
      "-------------------\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     31932\n",
      "           1       0.90      0.90      0.90     31684\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     63616\n",
      "   macro avg       0.90      0.90      0.90     63616\n",
      "weighted avg       0.90      0.90      0.90     63616\n",
      "\n",
      "-------------------\n",
      "confusion_matrix\n",
      "[[28623  3309]\n",
      " [ 3041 28643]]\n",
      "-------------------\n",
      "CPU times: user 14.6 s, sys: 237 ms, total: 14.8 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "model_bow = load('data/models/bow_model.pkl')\n",
    "get_metrics(model_bow, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some issues\n",
    "- Ngrams get us some of the way with the sentiment\n",
    "- just one vector per sentence (document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word embeddings\n",
    "- convert each word into vector\n",
    "- map it into a vector space\n",
    "- allows for word representations with similar meaning to have a similar representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2vec\n",
    "Word2Vec has two methods:\n",
    "1. CBOW\n",
    "2. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"pictures/skipgra, and cbow.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4bde74ab00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"pictures/skipgra, and cbow.png\", width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets train one and have a look of how good it is at finding word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lumia', 0.7854803204536438),\n",
       " ('htc', 0.7818347811698914),\n",
       " ('iphone', 0.7195836305618286),\n",
       " ('sony', 0.6978476643562317),\n",
       " ('lenovo', 0.692030668258667),\n",
       " ('huawei', 0.6810896396636963),\n",
       " ('bærbar', 0.6735221147537231),\n",
       " ('tablet', 0.6730877161026001),\n",
       " ('mobiltelefon', 0.6699701547622681),\n",
       " ('ipod', 0.6555711627006531)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "word2vec_model = gensim.models.Word2Vec.load(\"data/models/word2vec.model\")\n",
    "w1 = 'nokia'\n",
    "word2vec_model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FastText\n",
    "- Framework with word2vecs cbow implemented \n",
    "  - ngrams (both on words and characters)\n",
    "  - size (size of the context window)\n",
    "  - dim (size of the vector)\n",
    "- Each word is represented as a bag of character n-grams\n",
    "    - ngrams = 3\n",
    "    - where = <wh, whe, her, ere, re>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score\n",
      "0.9457883754461629\n",
      "-------------------\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95     31932\n",
      "           1       0.96      0.93      0.95     31684\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     63616\n",
      "   macro avg       0.95      0.95      0.95     63616\n",
      "weighted avg       0.95      0.95      0.95     63616\n",
      "\n",
      "-------------------\n",
      "confusion_matrix\n",
      "[[30552  1380]\n",
      " [ 2066 29618]]\n",
      "-------------------\n",
      "CPU times: user 49min 20s, sys: 9.16 s, total: 49min 29s\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from src.model.fasttext import fasttext_pipeline \n",
    "X_train,X_test,y_train,y_test = train_test_split(df_trust[['text']], df_trust['y'], random_state=42 )\n",
    "model_fasttext = fasttext_pipeline(X_train, y_train)\n",
    "get_metrics(model_fasttext, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transferlearning\n",
    "- lets take something that we have learned somewhere and apply it to a new domain\n",
    "- why this?\n",
    "    - we do not always have a ton of data\n",
    "    - The model do not need to learn from scratch\n",
    "    - Thus we save time\n",
    "    - We make (re)use of models built by very talented people\n",
    "    - the approach has proven itself in computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universal Language Model Fine-tuning for Text Classification (ULMFiT)\n",
    "- fast.ai\n",
    "- 15 may 2018\n",
    "- Make use of Language model approach\n",
    "  - A model which predicts the next word in a sentence\n",
    "      - e.g. \"jeg elsker pågen\" -> \"gifler\" \n",
    "  - has high understanding of semantics\n",
    "      - eg. \" så det der kommer ...\" != \"jeg så en flot sol opgang\"\n",
    "  - has a high understanding of grammar\n",
    "- Data = wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"200\"\n",
       "            src=\"pictures/ulmfit.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4bddbf6e48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"pictures/ulmfit.png\", width=600, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Status on a Danish ulmfit model\n",
    "- trained for 24 hours on Danish wikipedia\n",
    "- 20000 + wikipedia articles \n",
    "- 60000 + tokens\n",
    "\n",
    "Let's take a look at the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# models:\n",
    "- BOW\n",
    "    - 90% roc auc\n",
    "- Word2vec\n",
    "    - 94,5% roc auc\n",
    "- Transerlearning\n",
    "    - 81 % roc auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "------------------------------------------------------\n",
    "- BoW\n",
    "  - pros\n",
    "      - Robust towards its specific task\n",
    "      - High transparecy\n",
    "      - Quick to code\n",
    "      - easy to install\n",
    "  - cons\n",
    "      - \"only\" counts word\n",
    "      - loses semantics\n",
    "      - slow to train  \n",
    "------------------------------------------------------\n",
    "- Word2vec(FastText)\n",
    "    - pros\n",
    "        - take semantics into account\n",
    "        - is fast\n",
    "        - easy to install \n",
    "    - cons\n",
    "        - transparency is a bit of a blur\n",
    "        - needs a good portion of data  \n",
    "------------------------------------------------------\n",
    "- Transferlearning (ULMFIT)\n",
    "    - pros\n",
    "        - state of the art performanche (they say)\n",
    "        - have the potential to save a lot of time\n",
    "    - cons\n",
    "        - hard to code and install\n",
    "        - initial training takes a long time\n",
    "        - Black Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# KISS\n",
    "***Keep It Simple Stupid*** can be quite clever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Questions?\n",
    "\n",
    "Thank you for your time."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
