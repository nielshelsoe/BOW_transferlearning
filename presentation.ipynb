{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back Of Words --> Transferlearning\n",
    "by Niels Helsø \n",
    "git = https://github.com/slein89/BOW_transferlearning  \n",
    "linkin = https://www.linkedin.com/in/nielshelsoe/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "1. Introduktion\n",
    "2. BoW (Back Of Words)\n",
    "3. Word embedings\n",
    "4. Transferlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Code, articels, turtorials and stuf\n",
    "Git url = https://github.com/slein89/BOW_transferlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduktion\n",
    "- Who am i?\n",
    "    - Name = Niels\n",
    "    - Age = 29\n",
    "    - Education = cand. it\n",
    "    - Favorite guilty plasure = paagen gifler\n",
    "    - have code in python for 2 years time\n",
    "    - Love NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Danish Business Authority\n",
    "- 550 people\n",
    "- Goal: The best conditions and framework for Danish companies\n",
    "- Data-Scinece team of 5\n",
    "- Profils:\n",
    "    - fine arts\n",
    "    - economis\n",
    "    - engineer\n",
    "    - phd\n",
    "    - it\n",
    "    - informations studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# why this talk?\n",
    "1. Give a overview of where NLP is going and where it have been\n",
    "2. Explain what the pro and cons are of three diferrent methods\n",
    "3. I hope to demystify NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# PLEASE ASK QUESTIONS \n",
    "There are not such a thing as dum questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BoW (Back Of Words)\n",
    "- one of the first NLP approach\n",
    "- convert tekst into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus=['Pydatacopenhagen is the best place to be tonight, yes tonight',\n",
    "'At Pydatacopenhagen every one is welcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 1 1 1 2 0 1]\n",
      " [1 0 0 1 1 1 0 1 0 0 0 1 0]]\n",
      "{'pydatacopenhagen': 7, 'is': 4, 'the': 8, 'best': 2, 'place': 6, 'to': 9, 'be': 1, 'tonight': 10, 'yes': 12, 'at': 0, 'every': 3, 'one': 5, 'welcome': 11}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "#the vector representation of the words (13)\n",
    "print( vectorizer.fit_transform(corpus).todense() )\n",
    "#count of the uniq words in corpus\n",
    "print( vectorizer.vocabulary_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ngrams\n",
    "- helps with preserve some meaning for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pydatacopenhagen', 'is')\n",
      "('is', 'the')\n",
      "('the', 'best')\n",
      "('best', 'place')\n",
      "('place', 'to')\n",
      "('to', 'be')\n",
      "('be', 'tonight,')\n",
      "('tonight,', 'yes')\n",
      "('yes', 'tonight')\n"
     ]
    }
   ],
   "source": [
    "corpus=['Pydatacopenhagen is the best place to be tonight, yes tonight',\n",
    "'At Pydatacopenhagen every one is welcome']\n",
    "from nltk import ngrams\n",
    "grams = ngrams(corpus[0].split(),2)\n",
    "for gram in grams:\n",
    "    print (gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data (thansk to Prayson Wilfred Daniel)\n",
    "- trustpilot data\n",
    "- 254464 (after making a equal distrubiton)\n",
    "- task: build a sentiment classefier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# %load src/preprocessing/text_pre.py\n",
    "import re\n",
    "import string\n",
    "def clean_text (df, row_name):\n",
    "    #lower all text\n",
    "    df[row_name] = df[row_name].str.lower()\n",
    "    #remove all numbers\n",
    "    df[row_name] = df[row_name].apply(lambda x: x.translate(str.maketrans('','','0123456789')))\n",
    "    #remove all special chareters\n",
    "    df[row_name] = df[row_name].apply(lambda x: re.sub('[\\W]+', ' ', x))\n",
    "    #make translation tabel where punctuation is removed and apply it to the text\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    df[row_name] = df[row_name].apply(lambda x: x.translate(table))\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of the to classes\n",
      "-----------------\n",
      "1    127232\n",
      "0    127232\n",
      "Name: y, dtype: int64\n",
      "\n",
      "Examples of text\n",
      "-----------------\n",
      "det var iorden \n",
      "nåede sikkert og hurtigt frem det var en god oplevelse\n",
      "vi har altid været glade for tryg og deres måde at håndtere forsikringssager på vi anmelder self heller ikke hvad som helst men til gengæld får vi altid en ærlig og redelig behandling uden problemer selv om tryg i perioder kan være lidt dyrere end andre forsikringsselskaber foretrækker vi dem fordi vi så heller ikke skal trækkes med diskussioner om udbetaling når uheldet er ude \n",
      "hurtig pakke lev til pakkeboksen\n",
      "bestilte tøj på nettet og fik pakken hurtigere end ventet og kunne endda følge den hele vejen \n"
     ]
    }
   ],
   "source": [
    "from src.data.load_data import load_trustpilot_data\n",
    "from src.preprocessing.text_pre import clean_text\n",
    "df_trust = load_trustpilot_data()\n",
    "df_trust = clean_text(df_trust, 'text')\n",
    "print('Amount of the to classes')\n",
    "print('-----------------')\n",
    "print(df_trust['y'].value_counts())\n",
    "print('')\n",
    "print('Examples of text')\n",
    "print('-----------------')\n",
    "for index,row in df_trust[:5].iterrows():\n",
    "    print(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_trust['text'], df_trust['y'], random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from src.preprocessing.metrics import get_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Metrics:\n",
    "- Roc auc\n",
    "- Confusion Matrix\n",
    "- Classification report (recall, precession, F1 score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### performanche on BoW\n",
    "lets se how BOW performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score\n",
      "0.9001972503652068\n",
      "-------------------\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     31932\n",
      "           1       0.90      0.90      0.90     31684\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     63616\n",
      "   macro avg       0.90      0.90      0.90     63616\n",
      "weighted avg       0.90      0.90      0.90     63616\n",
      "\n",
      "-------------------\n",
      "confusion_matrix\n",
      "[[28623  3309]\n",
      " [ 3041 28643]]\n",
      "-------------------\n",
      "CPU times: user 14.6 s, sys: 237 ms, total: 14.8 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "model_bow = load('data/models/bow_model.pkl')\n",
    "get_metrics(model_bow, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some issues\n",
    "- Ngrams get us some of the way\n",
    "- Stil just one vector per sentens (document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word embdings\n",
    "Word2Vec has two methods:\n",
    "1. CBOW\n",
    "2. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"pictures/skipgra, and cbow.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4bde74ab00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"pictures/skipgra, and cbow.png\", width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets train one and have a look of how good it is at finding word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lumia', 0.7854803204536438),\n",
       " ('htc', 0.7818347811698914),\n",
       " ('iphone', 0.7195836305618286),\n",
       " ('sony', 0.6978476643562317),\n",
       " ('lenovo', 0.692030668258667),\n",
       " ('huawei', 0.6810896396636963),\n",
       " ('bærbar', 0.6735221147537231),\n",
       " ('tablet', 0.6730877161026001),\n",
       " ('mobiltelefon', 0.6699701547622681),\n",
       " ('ipod', 0.6555711627006531)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "word2vec_model = gensim.models.Word2Vec.load(\"data/models/word2vec.model\")\n",
    "w1 = 'nokia'\n",
    "word2vec_model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FastText\n",
    "- Framework with word2vecs cbow implementet \n",
    "  - ngrams (both on words and characters)\n",
    "  - size (size of the context window)\n",
    "  - dim (size of the vector)\n",
    "- Each word is represented as a bag of character n-grams\n",
    "    - ngrams = 3\n",
    "    - where = <wh, whe, her, ere, re>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score\n",
      "0.9457883754461629\n",
      "-------------------\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95     31932\n",
      "           1       0.96      0.93      0.95     31684\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     63616\n",
      "   macro avg       0.95      0.95      0.95     63616\n",
      "weighted avg       0.95      0.95      0.95     63616\n",
      "\n",
      "-------------------\n",
      "confusion_matrix\n",
      "[[30552  1380]\n",
      " [ 2066 29618]]\n",
      "-------------------\n",
      "CPU times: user 49min 20s, sys: 9.16 s, total: 49min 29s\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from src.model.fasttext import fasttext_pipeline \n",
    "X_train,X_test,y_train,y_test = train_test_split(df_trust[['text']], df_trust['y'], random_state=42 )\n",
    "model_fasttext = fasttext_pipeline(X_train, y_train)\n",
    "get_metrics(model_fasttext, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transferlearning\n",
    "- lets take somthing that we have learn somwhere else and apply it to a new domain\n",
    "- why this?\n",
    "    - we do not always have a ton of data\n",
    "    - The model do not need to learn from sratch\n",
    "    - Thus we save time\n",
    "    - We make use of the models there are bulid by very talented people\n",
    "    - the apporach have proven it self in computer vission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Universal Language Model Fine-tuning for Text Classification (ULMFiT)\n",
    "- fast.ai\n",
    "- 15 may 2018\n",
    "- Make use of Language model apporach\n",
    "  - A model which predict the next word in a sentence\n",
    "      - e.g. \"jeg elsker pågen\" -> \"gifler\" \n",
    "  - have high understading of semantic\n",
    "      - eg. \" så det der kommer ...\" != \"jeg så en flot sol opgang\"\n",
    "  - have a high understading of grammatic\n",
    "- Data = wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"200\"\n",
       "            src=\"pictures/ulmfit.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4bddbf6e48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"pictures/ulmfit.png\", width=600, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Status on a danish ulmfit model\n",
    "- traind for 24 hours on danish wikipidia\n",
    "- 20000 + wikipidia articels \n",
    "- 60000 + tokens\n",
    "- has for the now a ROC on 80% (train=2400/test=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets have a look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "- BoW\n",
    "  - pros\n",
    "      - Robust towards its specific task\n",
    "      - High transparity\n",
    "      - Quick to code\n",
    "  - cons\n",
    "      - \"only\" counts word\n",
    "      - loses semantics\n",
    "      - slow to train\n",
    "- Word2vec(FastText)\n",
    "    - pros\n",
    "        - take semantics into acount\n",
    "        - is fast\n",
    "    - cons\n",
    "        - transparity is a bit of a blur\n",
    "        - needs a good purtion of data\n",
    "- Transferlearning (ULMFIT)\n",
    "    - pros\n",
    "        - state of the art performanche (the say)\n",
    "        - have the potintial to save a lot of time\n",
    "    - cons\n",
    "        - hard to code\n",
    "        - initial training take a lot of time\n",
    "        - Black Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# KISS\n",
    "***Keep It Simpel and Stupid***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Questions?\n",
    "\n",
    "Thanks for your time."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
